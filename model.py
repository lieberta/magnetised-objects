import torchimport torch.nn as nnimport torch.nn.functional as Fimport pytorch_lightning as plclass DipolNet(pl.LightningModule):    def __init__(self):        super(DipolNet, self).__init__()        # Convolutional layers, Input: (batch, 12,3)        self.conv1 = nn.Conv2d(in_channels = 1, out_channels= 16, kernel_size=(1,3))        # Fully connected layers, here different layer sizes and amounts of layers should be tested        # After Convolution thoes 16 vectors        # The more complex the correlation between those 12 vectors and the Output        self.layersize = 64 # maybe try 128,256, 512 and evaluate the performance of the system        self.fc1 = nn.Linear(16 * 12, self.layersize * 2)        self.fc2 = nn.Linear(self.layersize*2, self.layersize*2)        self.fc3 = nn.Linear(self.layersize * 2, self.layersize)        #Output should be Orientation (dim = 2) and magnet strength (dim = 1)        self.fc4 = nn.Linear(self.layersize, 3)    def forward(self, x):        # Add a dimension for Input Channels from (batch, 12, 3) to (batch, 1, 12, 3)        x = x.unsqueeze(1)        # This Conv layer takes the (12,3) tensor and processes it into 16 different 12,1 vectors        # The goal is to extract interconnections between those 12 vectors        # F.relu is the activation function        x = F.relu(self.conv1(x))        # Flatten x such that the fully connected layer is able to process the tensor        x = x.view(x.size(0), -1)        # Fully connected layers        x = F.relu(self.fc1(x))        x = F.relu(self.fc2(x))        x = F.relu(self.fc3(x))        x = self.fc4(x)        # Some ideas how to improve the system by defining the range of the output        # Put sigmoid function onto the orientation such that it can only be between 0 and 360:        #orientation_output = torch.sigmoid(x[:, :2]) * 360        # Put tanh activation function onto third output: max_value and min_value is the desired range of the magnetization strength        #magnetization_output = (torch.tanh(x[:, 2]) + 1) * (max_value - min_value) / 2 + min_value        # combine those to one output (unsqueeze is adding a dimension at index 1, from (batch_size) to (batch_size,1)        #x = torch.cat((orientation_output, magnetization_output.unsqueeze(1)), dim=1)        return x# Definition for Pytorch Lightning Trainer    def training_step(self, batch, batch_idx):        x, y = batch        y_hat = self(x)        loss = F.mse_loss(y_hat, y)        self.log('train_loss', loss, on_step=True)        return loss    def validation_step(self, batch, batch_idx):        x, y = batch        y_hat = self(x)        loss = F.mse_loss(y_hat, y)        self.log('val_loss', loss, on_step=True)        return loss    def configure_optimizers(self):        return torch.optim.Adam(self.parameters(), lr=0.001)# Here just a random tensor is testedif __name__ == '__main__': model = DipolNet() x = torch.rand(1,12,3) y = model(x)